OCR implementation of HPCG
Author David S. Scott

The HPCG benchmark computes 50 iterations of the preconditioned conjugate gradient algorithm to solve Ax = b,
where A is a sparse matrix derived from a 27 point (3x3x3) stencil on a 3D grid of points.
See https://software.sandia.gov/hpcg/ for the C++ benchmark code.

The diagonal elements of A are set to 26 and all of the nonzero off diagonal elements are set to -1.  At the edge of the global grid, some of the -1s are missing but
the diagonal value is still set to 26 so the matrix is not singular.  The right hand side b is set to the row sums of the matrix so the solution is all 1s.

The matrix MUST be represented as a linear array of non zero elements with corresponding column indices.  The algorithm requires a sparse matrix vector product and 3 inner products
in addition to the precondioner.


The code uses labeled GUIDs to do the halo exchanges.
It uses the reduction library (which also uses labeled GUIDs) to compute global sums.

Input Parameters
The code accepts 0, 3, 4, 5, or 6  input parameters:
If an input parameter is not specified the code has a #define value as a default

npx, npy, npz	the grid of "ranks" is sized npx X npy X npz [current default 3x4x5 but that might change to meet Jenkins requirements]
m		each rank has an MxMxM chunk of the global grid.  M must be a multiple of 16 and will be rounded up if needed [default 16]
t		the maximum number of iterations [default 50]
debug		if debug=1 prints some information, debug=2 prints a LOT  [default 0]


Parallelism
The code assumes the grid consists of N = NPX x NPY x NPZ blocks of size MxMxM.
One worker EDT per block.


Inner products are implemented using the reduction library with vectors of length one and reductionOperator REDUCTION_F8_ADD (which uses Labeled GUIDs and Channel Events)

The matrix vector product requires a halo exchange in which boundary values are traded with up to 26 neighbors.

The preconditioner is 4 levels of multigrid using symmetric Gauss-Seidel sweeps as the smoother which also needs halo exchanges
There is a #define in the code for the preconditioner.
If commented out, the conjugate gradient algorithm runs without the preconditioning each iteration is faster but the convergence is slower


Events:
The code uses labeled GUIDs to exchange channel events for the halo exchanges
It uses ordinary events for returning to the next phase.

The reduction library uses labeled guids for initialization and channel events thereafter


Structure of the Code

mainEdt:	creates the shared datablock and launches realmainEdt
realmainEdt:	initializes the shared datablock N initEdts with the shared block and "myrank" as a parameter

Each of N copies of:

InitEdt:	creates the private block, the reduction private block and launches hpcgInitEdt
hpcgInitEdt:	populates the private blocks and launches hpcgEdt
hpcgInitChannel uses the available labeled GUIDs to exchange channel events for the halo exchanges

hpcgEdt:	written in a continuation style with a "phase" and a switch statement, each phase clones itself for the next phase

Phase0:		initialize and launch global sum for rtr
Phase1:		check convergence and launch mgEdt (preconditioner)
Phase2: 	launch global sum for rtz
Phase3:		launch halo exchange and spmv (sparse matrix vector product Ap)
Phase4:		launch global sum for pAp
Phase5:		launch global sum for rtr and return to phase 1 for next iteration

haloExchange:	launches pack and unpack
pack:	    packs boundary values into small datablocks and satisfies events.  Then satisfies an event to help wake up spmv
unpack:		depends on the 26 events to receive the neighbors buffer blocks.  Puts the values in the vector.  Satisfies an event to help wake up spmv
spmv:		actually does the matrix-vector product and satisfies an event to wake up the next phase


mgEdt		also written in a continuation style with both phase(5) and level (4).

Phase0:		launch Gauss-Seidel smoothing (starting with z=0) which depends on halo exchange (and pack and unpack) and then returns to phase 1
                   except at the lowest level when you return to Phase 3 of the higher level
Phase1:		launch spmv which depends on Halo Exchange (and pack and unpack) and returns to Phase 2
Phase2:		launch recursive call on smaller grid
Phase3: 	launch smoother with halo exchange (after return from lower level)
Phase4:		returns to next higher level (or Phase 3 of hpcg)


The algorithm will terminate, either after t (default 50) iterations or if the residual gets small enough.
Each worker then computes its deviation from the answer (which is all ones) and then launches a "final" global sum that returns only to wrapUpEdt,
which prints the sum and calls ocrShutdown().

There is a go.sh script whose active line refers to a nodefile16.
Both the file and and pointer to it in go.sh will have to be modified for the machine you are running on.
